{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from dual_network import Dual3DCNN3, Dual3DCNN4, Dual3DCNN5, FlexibleDual3DCNN\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import glob\n",
    "from utilities import create_list_from_master_json, read_json_file, split_data\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "from utilities import list_patient_folders, prepare_data_nrrd, split_data\n",
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Spacingd, ScaleIntensityd, SpatialPadd, CenterSpatialCropd, ScaleIntensityRanged\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.transforms import LoadImaged\n",
    "from monai.data.image_reader import ITKReader\n",
    "from monai.data import SmartCacheDataset\n",
    "import random\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 336\n",
      "Number of validation samples: 102\n",
      "Number of test samples: 65\n",
      "503\n"
     ]
    }
   ],
   "source": [
    "# data_path_NEW = '/data/shahpouriz/DBP_newDATA/nrrd/oneCTperPatients/proton'\n",
    "# data_path = '/data/shahpouriz/DBP_DATA_total/nrrd/oneCTperPatinet/proton'\n",
    "data_path = '/data/shahpouriz/DBP_DATA_total/nrrd/oneOPTZperPatinet/proton'\n",
    "\n",
    "patient_list_NEW = list_patient_folders(data_path)\n",
    "# Shuffle patient list if you want randomness\n",
    "random.seed(42)  # You can choose any number as the seed\n",
    "random.shuffle(patient_list_NEW)\n",
    "\n",
    "# Define split sizes\n",
    "total_patients = len(patient_list_NEW)\n",
    "train_size = int(total_patients * 0.70)\n",
    "val_size = int(total_patients * 0.20)\n",
    "# The rest will be for the test set\n",
    "\n",
    "# Split the patient list\n",
    "train_patients = patient_list_NEW[:train_size]\n",
    "val_patients = patient_list_NEW[train_size:train_size + val_size]\n",
    "test_patients = patient_list_NEW[train_size + val_size:]\n",
    "\n",
    "train_pct, train_rct, train_pos = prepare_data_nrrd(data_path, train_patients)\n",
    "val_pct, val_rct, val_pos = prepare_data_nrrd(data_path, val_patients)\n",
    "test_pct, test_rct, test_pos = prepare_data_nrrd(data_path, test_patients)\n",
    "\n",
    "# Create dictionaries for each dataset\n",
    "train_data = [{\"plan\": img, \"repeat\": tar, \"pos\": pos} for img, tar, pos in zip(train_pct, train_rct, train_pos)]\n",
    "val_data = [{\"plan\": img, \"repeat\": tar, \"pos\": pos} for img, tar, pos in zip(val_pct, val_rct, val_pos)]\n",
    "test_data = [{\"plan\": img, \"repeat\": tar, \"pos\": pos} for img, tar, pos in zip(test_pct, test_rct, test_pos)]\n",
    "\n",
    "\n",
    "# Check the lengths of the sets\n",
    "print(\"Number of training samples:\", len(train_data))\n",
    "print(\"Number of validation samples:\", len(val_data))\n",
    "print(\"Number of test samples:\", len(test_data))\n",
    "print(len(test_data)+len(val_data)+len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 268/268 [00:17<00:00, 15.20it/s]\n",
      "Loading dataset: 100%|██████████| 81/81 [00:03<00:00, 21.90it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim = 128\n",
    "size = (dim, dim, dim)\n",
    "pixdim = (3.0, 3.0, 3.0)\n",
    "transforms = Compose([\n",
    "        LoadImaged(keys=[\"plan\", \"repeat\"], reader=ITKReader()),\n",
    "        EnsureChannelFirstd(keys=[\"plan\", \"repeat\"]),\n",
    "        # ScaleIntensityd(keys=[\"plan\", \"repeat\"]),\n",
    "        Spacingd(keys=[\"plan\", \"repeat\"], pixdim=pixdim, mode='trilinear'),\n",
    "        SpatialPadd(keys=[\"plan\", \"repeat\"], spatial_size=size, mode='constant'),  # Ensure minimum size\n",
    "        CenterSpatialCropd(keys=[\"plan\", \"repeat\"], roi_size=size),  # Ensure uniform size\n",
    "    ])\n",
    "\n",
    "\n",
    "train_ds = CacheDataset(data=train_data, transform=transforms, cache_rate=0.8, num_workers=1)\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "val_ds = CacheDataset(data=val_data, transform=transforms, cache_rate=0.8, num_workers=1)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:28:13,501] A new study created in RDB with name: DBP_optimize_1opt\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [8, 16, 32] which is of type list.\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [16, 32, 64] which is of type list.\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [32, 64, 128] which is of type list.\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [16, 32, 64, 128] which is of type list.\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [32, 64, 128, 256] which is of type list.\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [16, 32, 128, 256, 512] which is of type list.\n",
      "[I 2024-03-18 16:21:58,306] Trial 0 finished with value: 0.6664595730824159 and parameters: {'architecture': [32, 64, 128], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.6664595730824159.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 16:41:09,179] Trial 1 finished with value: 0.6658520485547271 and parameters: {'architecture': [16, 32, 64], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 1 with value: 0.6658520485547271.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:04:41,216] Trial 2 finished with value: 0.6549073285331913 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n",
      "[I 2024-03-18 17:24:55,490] Trial 3 finished with value: 0.6587223221054849 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:39:51,896] Trial 4 finished with value: 0.6556631134567308 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:00:56,196] Trial 5 finished with value: 14.068849175584083 and parameters: {'architecture': [32, 64, 128], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:10:40,974] Trial 6 finished with value: 0.6585140769899476 and parameters: {'architecture': [16, 32, 64], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:20:26,702] Trial 7 finished with value: 0.6678790263820202 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n",
      "[I 2024-03-18 18:30:27,363] Trial 8 finished with value: 0.657139484040147 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:51:30,028] Trial 9 finished with value: 0.6648469528448129 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:58:02,927] Trial 10 finished with value: 0.6553718451337487 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:04:38,124] Trial 11 finished with value: 0.6551381504784027 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:11:02,540] Trial 12 finished with value: 0.6549187977341753 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:17:52,173] Trial 13 finished with value: 0.6551077469416401 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:31:51,634] Trial 14 finished with value: 0.6550704450255224 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:53:40,001] Trial 15 finished with value: 0.655120197135736 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:00:22,373] Trial 16 finished with value: 0.6550465616963658 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:08:53,560] Trial 17 finished with value: 0.6549160264468953 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:17:23,561] Trial 18 finished with value: 0.655109089050515 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:25:54,465] Trial 19 finished with value: 0.6551250536177381 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:34:26,801] Trial 20 finished with value: 0.6586154190019942 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:38:40,924] Trial 21 finished with value: 0.6552167112993843 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:47:12,169] Trial 22 finished with value: 0.6552793784980097 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:51:25,476] Trial 23 finished with value: 0.6552499874914978 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 20:59:56,796] Trial 24 finished with value: 0.6550951955986082 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 21:06:23,828] Trial 25 finished with value: 0.6550469642103303 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 21:18:58,853] Trial 26 finished with value: 0.6658715687169438 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 21:30:56,222] Trial 27 finished with value: 0.6551516699403817 and parameters: {'architecture': [32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 21:35:12,225] Trial 28 finished with value: 0.6551876446487856 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 21:47:10,729] Trial 29 finished with value: 0.6649801884917657 and parameters: {'architecture': [32, 64, 128], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 21:55:43,127] Trial 30 finished with value: 0.6575349676696693 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 22:02:10,014] Trial 31 finished with value: 0.6552127792189518 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 22:08:36,058] Trial 32 finished with value: 0.6552147080610488 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 22:15:02,767] Trial 33 finished with value: 0.6552880224497879 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 22:21:29,708] Trial 34 finished with value: 0.655385406140019 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 22:27:43,665] Trial 35 finished with value: 0.6639200647843216 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 22:36:14,740] Trial 36 finished with value: 0.802285658779974 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 22:44:46,166] Trial 37 finished with value: 0.6549972638341726 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 22:53:18,028] Trial 38 finished with value: 0.6653853578951813 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:01:48,908] Trial 39 finished with value: 0.6606180240747099 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:10:20,719] Trial 40 finished with value: 0.6551494767122409 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:16:35,106] Trial 41 finished with value: 0.6550637823620847 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:28:30,379] Trial 42 finished with value: 0.6551217200307577 and parameters: {'architecture': [32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:37:02,118] Trial 43 finished with value: 0.6552931382229515 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:41:16,409] Trial 44 finished with value: 0.6584289516778845 and parameters: {'architecture': [8, 16, 32], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:47:43,565] Trial 45 finished with value: 0.6560063167822128 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 23:56:15,493] Trial 46 finished with value: 0.6659915865457151 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:08:51,953] Trial 47 finished with value: 0.6551317035713616 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:13:07,589] Trial 48 finished with value: 0.6552413821804757 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:19:24,350] Trial 49 finished with value: 0.6550271897120219 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:25:41,374] Trial 50 finished with value: 0.6552377111260214 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:31:58,014] Trial 51 finished with value: 0.6551031223772203 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:38:15,208] Trial 52 finished with value: 0.655252412728527 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:44:31,655] Trial 53 finished with value: 0.6552200761130628 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:53:03,326] Trial 54 finished with value: 0.6552731712878335 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 00:57:17,305] Trial 55 finished with value: 0.6553279170218635 and parameters: {'architecture': [8, 16, 32], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 01:05:49,668] Trial 56 finished with value: 0.6584447517844976 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 01:18:25,373] Trial 57 finished with value: 0.6552249018431586 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 01:24:54,455] Trial 58 finished with value: 0.6677555520105741 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 01:33:25,312] Trial 59 finished with value: 0.6551155144093084 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n",
      "[I 2024-03-19 01:45:22,977] Trial 60 finished with value: 0.6554267390028519 and parameters: {'architecture': [32, 64, 128], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 01:51:49,965] Trial 61 finished with value: 0.6550804742922386 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 01:58:16,892] Trial 62 finished with value: 0.6584671225544869 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:04:44,306] Trial 63 finished with value: 0.6552393135304252 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:11:10,657] Trial 64 finished with value: 0.6552589957912763 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:15:24,218] Trial 65 finished with value: 0.6552616094841677 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:23:55,624] Trial 66 finished with value: 0.6554060320132503 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:30:21,634] Trial 67 finished with value: 0.655110408953738 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:36:34,503] Trial 68 finished with value: 0.6550600408010331 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:45:06,873] Trial 69 finished with value: 0.6710522218023007 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:49:20,098] Trial 70 finished with value: 0.6551462043734158 and parameters: {'architecture': [8, 16, 32], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 02:55:33,794] Trial 71 finished with value: 0.6551560473208334 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 03:01:47,039] Trial 72 finished with value: 0.6552528244461499 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 03:07:59,862] Trial 73 finished with value: 0.6551512969548211 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 03:14:13,787] Trial 74 finished with value: 0.6552511445228376 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 03:22:45,323] Trial 75 finished with value: 0.6551795002693931 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 03:35:21,020] Trial 76 finished with value: 0.6553497281217692 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 03:41:50,546] Trial 77 finished with value: 0.6553615149402735 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n",
      "[I 2024-03-19 03:53:49,806] Trial 78 finished with value: 0.6552314900373127 and parameters: {'architecture': [32, 64, 128], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:00:06,179] Trial 79 finished with value: 0.6553263399457815 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:08:37,814] Trial 80 finished with value: 0.664732540326476 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:14:51,696] Trial 81 finished with value: 0.6552061621172756 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:21:04,853] Trial 82 finished with value: 0.6550725993572497 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 2 with value: 0.6549073285331913.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:27:19,480] Trial 83 finished with value: 0.6548776531993759 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 83 with value: 0.6548776531993759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:33:32,511] Trial 84 finished with value: 0.6551458582866425 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 83 with value: 0.6548776531993759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:37:46,796] Trial 85 finished with value: 0.6550150745566569 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 83 with value: 0.6548776531993759.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:42:00,724] Trial 86 finished with value: 0.6548317122437498 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 86 with value: 0.6548317122437498.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:46:14,030] Trial 87 finished with value: 0.6555548533797264 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 86 with value: 0.6548317122437498.\n",
      "[I 2024-03-19 04:50:27,569] Trial 88 finished with value: 0.6552455888001942 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 86 with value: 0.6548317122437498.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:54:41,610] Trial 89 finished with value: 0.6551788803189993 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 86 with value: 0.6548317122437498.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 04:58:56,161] Trial 90 finished with value: 0.6549041513901424 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 86 with value: 0.6548317122437498.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:03:10,300] Trial 91 finished with value: 0.6555787040760704 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 86 with value: 0.6548317122437498.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:07:23,751] Trial 92 finished with value: 0.6551877726599866 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 86 with value: 0.6548317122437498.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:11:37,440] Trial 93 finished with value: 0.654657388306862 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:15:51,240] Trial 94 finished with value: 0.6554531670887681 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:20:05,103] Trial 95 finished with value: 0.6549691147554446 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:24:18,934] Trial 96 finished with value: 0.6549013604172597 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:28:32,255] Trial 97 finished with value: 0.6550514750986123 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:32:45,451] Trial 98 finished with value: 0.6552926209554368 and parameters: {'architecture': [8, 16, 32], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n",
      "[I 2024-03-19 05:36:59,552] Trial 99 finished with value: 0.6657801692410573 and parameters: {'architecture': [8, 16, 32], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:41:13,236] Trial 100 finished with value: 0.6549625348518876 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:45:26,859] Trial 101 finished with value: 0.6550448225336332 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:49:39,980] Trial 102 finished with value: 0.6554296910397562 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:53:53,091] Trial 103 finished with value: 0.6551127722121629 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 05:58:06,438] Trial 104 finished with value: 0.6551769004732955 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:02:20,197] Trial 105 finished with value: 0.6552559840036374 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:06:33,747] Trial 106 finished with value: 0.6554897713529713 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:10:47,679] Trial 107 finished with value: 0.6554974535224485 and parameters: {'architecture': [8, 16, 32], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:15:01,014] Trial 108 finished with value: 0.6553978904643479 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:23:32,871] Trial 109 finished with value: 0.65533333363048 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:27:47,401] Trial 110 finished with value: 0.655289269216797 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:32:01,460] Trial 111 finished with value: 0.6551505741024134 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:36:15,574] Trial 112 finished with value: 0.6547355437833889 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:40:29,395] Trial 113 finished with value: 0.6554334764460138 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:44:43,161] Trial 114 finished with value: 0.6548826233940381 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:48:56,565] Trial 115 finished with value: 0.6552832148896128 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:53:10,255] Trial 116 finished with value: 0.6553923580065077 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 06:57:23,875] Trial 117 finished with value: 0.6554925313182905 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 07:01:36,765] Trial 118 finished with value: 0.6552355203996686 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 07:14:12,313] Trial 119 finished with value: 0.6649966680218794 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 07:18:29,750] Trial 120 finished with value: 0.655017456660668 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 07:27:01,290] Trial 121 finished with value: 0.655272574145712 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 07:35:32,905] Trial 122 finished with value: 0.6550272235500754 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 07:39:47,217] Trial 123 finished with value: 0.6556231027615129 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 07:51:43,823] Trial 124 finished with value: 0.6550403059661096 and parameters: {'architecture': [32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n",
      "[I 2024-03-19 08:00:16,688] Trial 125 finished with value: 0.6706522611134192 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:04:30,026] Trial 126 finished with value: 0.6553811650065815 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:08:44,128] Trial 127 finished with value: 0.6552242046112523 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:12:58,140] Trial 128 finished with value: 0.6551752506079627 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:21:30,103] Trial 129 finished with value: 0.6549890110926593 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:25:44,252] Trial 130 finished with value: 0.6557098993775892 and parameters: {'architecture': [8, 16, 32], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:34:16,428] Trial 131 finished with value: 0.6552899865543141 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:42:48,182] Trial 132 finished with value: 0.655063944028727 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:51:20,886] Trial 133 finished with value: 0.65506317207188 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 08:59:52,813] Trial 134 finished with value: 0.655110617588256 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 09:08:24,806] Trial 135 finished with value: 0.6553961688689157 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 09:12:38,516] Trial 136 finished with value: 0.6552923685122355 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 09:16:52,607] Trial 137 finished with value: 0.6548272868362712 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 09:21:06,457] Trial 138 finished with value: 0.6552377059763553 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-19 09:25:19,662] Trial 139 finished with value: 0.6550741263427863 and parameters: {'architecture': [8, 16, 32], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 93 with value: 0.654657388306862.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Loss:  0.654657388306862\n",
      "  Params: \n",
      "    architecture: [8, 16, 32]\n",
      "    lr: 0.001\n",
      "    optimizer: Adam\n",
      "    loss_function: L1Loss\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import functools\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "lambda_reg = 0.0  # for L1 regularization\n",
    "weight_decay = 0.0  # for L2 regularization\n",
    "\n",
    "\n",
    "def objective(trial, device, final_epoch):\n",
    "    # Predefined layer configurations\n",
    "    architectures = [\n",
    "        [8, 16, 32],   # Small network\n",
    "        [16, 32, 64],  # Small network\n",
    "\n",
    "        [32, 64, 128],  # Medium network\n",
    "        [16, 32, 64, 128],\n",
    "\n",
    "        [32, 64, 128, 256],  # Larger network\n",
    "        [16, 32, 128, 256, 512]  # Even larger network\n",
    "    ]\n",
    "    chosen_architecture = trial.suggest_categorical('architecture', architectures)\n",
    "    \n",
    "    model = FlexibleDual3DCNN(chosen_architecture).to(device)\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    lr = trial.suggest_categorical('lr', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam']) #, 'RMSprop', 'SGD'])\n",
    "    loss_function = trial.suggest_categorical('loss_function', ['MSELoss', 'L1Loss']) #, 'SmoothL1Loss'])\n",
    "\n",
    "    # Select optimizer\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Select loss function\n",
    "    if loss_function == 'MSELoss':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_function == 'L1Loss':\n",
    "        criterion = nn.L1Loss()\n",
    "    elif loss_function == 'SmoothL1Loss':\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(final_epoch):\n",
    "        model.train()\n",
    "        loss_list = []\n",
    "        for batch_data in train_loader:\n",
    "            pCT, rCT = batch_data[\"plan\"].to(device), batch_data[\"repeat\"].to(device)\n",
    "            reg = batch_data[\"pos\"].clone().detach().requires_grad_(True).to(device)  # If gradients are required for 'reg'\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(pCT, rCT)\n",
    "            loss = criterion(output, reg)\n",
    "\n",
    "\n",
    "            # L1 Regularization\n",
    "            l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "            for name, param in model.named_parameters():\n",
    "                # print(name, param)\n",
    "                if 'weight' in name:  # Apply L1 only to weights, not biases\n",
    "                    l1_reg = l1_reg + torch.norm(param, 1)\n",
    "            \n",
    "            # Combine loss with L1 regularization\n",
    "            loss = loss + lambda_reg * l1_reg\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            mean_tot_loss = np.mean(loss_list)\n",
    "            # print(f'Epoch: {epoch}/{final_epoch}, Batch: {i+1}/{len(train_loader)}, Loss_avg: {mean_tot_loss}')\n",
    "\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                pCT_val, rCT_val = batch_data[\"plan\"].to(device), batch_data[\"repeat\"].to(device)\n",
    "                reg_val = batch_data[\"pos\"].clone().detach().requires_grad_(True).to(device)  # If gradients are required for 'reg'\n",
    "\n",
    "                output_val = model(pCT_val, rCT_val)\n",
    "                loss_output_val = criterion(output_val, reg_val)\n",
    "\n",
    "                val_loss.append(loss_output_val.item())\n",
    "\n",
    "            mean_val_loss = np.mean(val_loss)\n",
    "            # print(f'Epoch [{epoch+1}/{final_epoch}], Validation Loss: {mean_val_loss:.4f}')\n",
    "\n",
    "            # Adjust learning rate\n",
    "            scheduler.step(mean_val_loss)\n",
    "    return mean_val_loss\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_epoch = 20 # The number of epochs\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize', study_name='DBP_optimize_1opt', storage='sqlite:///DBP_optimizer_1opt.db', load_if_exists=True)\n",
    "study.optimize(lambda trial: objective(trial, device, final_epoch), n_trials=140) # the number of trials as needed\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"  Loss: \", study.best_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
