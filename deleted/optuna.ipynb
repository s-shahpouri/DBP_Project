{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from dual_network import Dual3DCNN3, Dual3DCNN4, Dual3DCNN5, FlexibleDual3DCNN\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import glob\n",
    "from utilities import create_list_from_master_json, read_json_file, split_data\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "from utilities import list_patient_folders, prepare_data_nrrd, split_data\n",
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Spacingd, ScaleIntensityd, SpatialPadd, CenterSpatialCropd, ScaleIntensityRanged\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.transforms import LoadImaged\n",
    "from monai.data.image_reader import ITKReader\n",
    "from monai.data import SmartCacheDataset\n",
    "import random\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 65\n",
      "Number of validation samples: 17\n",
      "Number of test samples: 7\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "# data_path_NEW = '/data/shahpouriz/DBP_newDATA/nrrd/oneCTperPatients/proton'\n",
    "data_path = '/data/shahpouriz/DBP_DATA_total/nrrd/oneCTperPatinet/proton'\n",
    "\n",
    "patient_list_NEW = list_patient_folders(data_path)\n",
    "# Shuffle patient list if you want randomness\n",
    "random.shuffle(patient_list_NEW)\n",
    "\n",
    "# Define split sizes\n",
    "total_patients = len(patient_list_NEW)\n",
    "train_size = int(total_patients * 0.70)\n",
    "val_size = int(total_patients * 0.20)\n",
    "# The rest will be for the test set\n",
    "\n",
    "# Split the patient list\n",
    "train_patients = patient_list_NEW[:train_size]\n",
    "val_patients = patient_list_NEW[train_size:train_size + val_size]\n",
    "test_patients = patient_list_NEW[train_size + val_size:]\n",
    "\n",
    "train_pct, train_rct, train_pos = prepare_data_nrrd(data_path, train_patients)\n",
    "val_pct, val_rct, val_pos = prepare_data_nrrd(data_path, val_patients)\n",
    "test_pct, test_rct, test_pos = prepare_data_nrrd(data_path, test_patients)\n",
    "\n",
    "# Create dictionaries for each dataset\n",
    "train_data = [{\"plan\": img, \"repeat\": tar, \"pos\": pos} for img, tar, pos in zip(train_pct, train_rct, train_pos)]\n",
    "val_data = [{\"plan\": img, \"repeat\": tar, \"pos\": pos} for img, tar, pos in zip(val_pct, val_rct, val_pos)]\n",
    "test_data = [{\"plan\": img, \"repeat\": tar, \"pos\": pos} for img, tar, pos in zip(test_pct, test_rct, test_pos)]\n",
    "\n",
    "\n",
    "# Check the lengths of the sets\n",
    "print(\"Number of training samples:\", len(train_data))\n",
    "print(\"Number of validation samples:\", len(val_data))\n",
    "print(\"Number of test samples:\", len(test_data))\n",
    "print(len(test_data)+len(val_data)+len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset: 100%|██████████| 52/52 [00:05<00:00,  9.99it/s]\n",
      "Loading dataset: 100%|██████████| 13/13 [00:00<00:00, 17.76it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim = 128\n",
    "size = (dim, dim, dim)\n",
    "pixdim = (3.0, 3.0, 3.0)\n",
    "transforms = Compose([\n",
    "        LoadImaged(keys=[\"plan\", \"repeat\"], reader=ITKReader()),\n",
    "        EnsureChannelFirstd(keys=[\"plan\", \"repeat\"]),\n",
    "        # ScaleIntensityd(keys=[\"plan\", \"repeat\"]),\n",
    "        Spacingd(keys=[\"plan\", \"repeat\"], pixdim=pixdim, mode='trilinear'),\n",
    "        SpatialPadd(keys=[\"plan\", \"repeat\"], spatial_size=size, mode='constant'),  # Ensure minimum size\n",
    "        CenterSpatialCropd(keys=[\"plan\", \"repeat\"], roi_size=size),  # Ensure uniform size\n",
    "    ])\n",
    "\n",
    "\n",
    "train_ds = CacheDataset(data=train_data, transform=transforms, cache_rate=0.8, num_workers=1)\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=1)\n",
    "\n",
    "val_ds = CacheDataset(data=val_data, transform=transforms, cache_rate=0.8, num_workers=1)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:09:55,204] A new study created in RDB with name: DBP_optimize\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [8, 16, 32] which is of type list.\n",
      "Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains [16, 32, 64, 128] which is of type list.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:13:46,757] Trial 0 finished with value: 0.631725579390631 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:15:45,485] Trial 1 finished with value: 0.6514400508090415 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:18:29,775] Trial 2 finished with value: 0.6394943342927624 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:21:07,252] Trial 3 finished with value: 0.6345886320752256 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:23:54,643] Trial 4 finished with value: 0.6590982526540756 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:25:58,273] Trial 5 finished with value: 0.6728994119090631 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:27:58,596] Trial 6 finished with value: 0.6392401163192356 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:30:05,906] Trial 7 finished with value: 0.6383572716046783 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:33:21,330] Trial 8 finished with value: 0.6581473340663839 and parameters: {'architecture': [32, 64, 128], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:36:10,592] Trial 9 finished with value: 0.668754293994211 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:39:51,629] Trial 10 finished with value: 0.6361247137983275 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:43:51,633] Trial 11 finished with value: 0.6758010970319018 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 0 with value: 0.631725579390631.\n",
      "[I 2024-03-18 12:45:14,009] Trial 12 finished with value: 0.6892288466915488 and parameters: {'architecture': [8, 16, 32], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:48:31,966] Trial 13 finished with value: 0.637961786459474 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:49:59,464] Trial 14 finished with value: 0.6357974351164611 and parameters: {'architecture': [8, 16, 32], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:53:59,959] Trial 15 finished with value: 0.6381010066936997 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 0 with value: 0.631725579390631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 12:56:42,238] Trial 16 finished with value: 0.6689721210311879 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 0 with value: 0.631725579390631.\n",
      "[I 2024-03-18 13:00:24,477] Trial 17 finished with value: 0.6272189192035619 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 17 with value: 0.6272189192035619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:03:38,940] Trial 18 finished with value: 0.6433124554288738 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 17 with value: 0.6272189192035619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:07:30,552] Trial 19 finished with value: 0.6559626083163654 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 17 with value: 0.6272189192035619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:11:20,654] Trial 20 finished with value: 0.6497899475781357 and parameters: {'architecture': [32, 64, 128], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 17 with value: 0.6272189192035619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:15:12,581] Trial 21 finished with value: 0.6675839494256413 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 17 with value: 0.6272189192035619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:18:43,090] Trial 22 finished with value: 0.6310440821244436 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 17 with value: 0.6272189192035619.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:25:21,470] Trial 23 finished with value: 0.6103544108131352 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:28:07,501] Trial 24 finished with value: 0.6175790400627781 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:31:07,129] Trial 25 finished with value: 0.6409529287587193 and parameters: {'architecture': [16, 32, 64], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:34:22,857] Trial 26 finished with value: 0.6362148898051065 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:37:35,084] Trial 27 finished with value: 0.6151450626990375 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n",
      "[I 2024-03-18 13:40:51,082] Trial 28 finished with value: 0.6752009194563416 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:44:00,172] Trial 29 finished with value: 0.6273047163644258 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n",
      "[I 2024-03-18 13:45:25,664] Trial 30 finished with value: 0.6484899595379829 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:48:07,518] Trial 31 finished with value: 0.6499156546505058 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:55:21,179] Trial 32 finished with value: 0.6353617812342504 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 13:57:44,885] Trial 33 finished with value: 0.6348453738233623 and parameters: {'architecture': [8, 16, 32], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:00:20,585] Trial 34 finished with value: 0.6382271441466668 and parameters: {'architecture': [16, 32, 64], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:03:21,889] Trial 35 finished with value: 0.6437905798940098 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:08:51,454] Trial 36 finished with value: 0.6303484088138622 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:12:08,701] Trial 37 finished with value: 0.6187181958161733 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:14:44,422] Trial 38 finished with value: 0.6245100248385879 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:17:58,093] Trial 39 finished with value: 0.6214769678738188 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:20:40,000] Trial 40 finished with value: 0.6535476669669151 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:23:49,203] Trial 41 finished with value: 0.6527348682284355 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:26:02,830] Trial 42 finished with value: 0.6504474523312905 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:27:48,961] Trial 43 finished with value: 0.6236890714396449 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:31:08,261] Trial 44 finished with value: 0.6351411682279671 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:33:35,357] Trial 45 finished with value: 0.6502996889107368 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:36:26,286] Trial 46 finished with value: 0.6344224117038881 and parameters: {'architecture': [16, 32, 64], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:39:43,252] Trial 47 finished with value: 0.6390045673093375 and parameters: {'architecture': [16, 32, 64], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:44:15,528] Trial 48 finished with value: 0.6421506630037638 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:45:41,269] Trial 49 finished with value: 0.643540901296279 and parameters: {'architecture': [8, 16, 32], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:47:25,660] Trial 50 finished with value: 0.6388385186300558 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:49:45,235] Trial 51 finished with value: 0.6447524651885033 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:53:07,506] Trial 52 finished with value: 0.6349249736351126 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 14:56:32,379] Trial 53 finished with value: 0.6226536649114945 and parameters: {'architecture': [16, 32, 64], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00008: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:00:40,011] Trial 54 finished with value: 0.6379174522179014 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.0001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:03:57,790] Trial 55 finished with value: 0.6801112021374351 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:06:56,830] Trial 56 finished with value: 0.6385349550229662 and parameters: {'architecture': [16, 32, 64], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:13:04,121] Trial 57 finished with value: 0.6212280539467054 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:19:30,028] Trial 58 finished with value: 0.6335653513669968 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:26:34,954] Trial 59 finished with value: 0.6623089876683319 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:35:09,274] Trial 60 finished with value: 0.6213825805222287 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:45:06,697] Trial 61 finished with value: 0.6534136415404432 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 15:57:05,647] Trial 62 finished with value: 0.6250098391490824 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 16:07:09,774] Trial 63 finished with value: 0.6630630164461977 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 16:17:12,473] Trial 64 finished with value: 0.6294528333579793 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 23 with value: 0.6103544108131352.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 16:26:31,790] Trial 65 finished with value: 0.6035901544725194 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 16:33:52,552] Trial 66 finished with value: 0.6595611423254013 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 16:40:18,303] Trial 67 finished with value: 0.636230035301517 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n",
      "[I 2024-03-18 16:46:31,998] Trial 68 finished with value: 0.6855953585575608 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 16:53:30,847] Trial 69 finished with value: 0.6796575099613298 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 65 with value: 0.6035901544725194.\n",
      "[I 2024-03-18 16:55:39,869] Trial 70 finished with value: 0.6644516251981258 and parameters: {'architecture': [8, 16, 32], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:02:36,161] Trial 71 finished with value: 0.6652669227298569 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:06:39,448] Trial 72 finished with value: 0.610697795801303 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:11:30,898] Trial 73 finished with value: 0.6401828076033032 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:18:46,160] Trial 74 finished with value: 0.6281620093128261 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:22:04,543] Trial 75 finished with value: 0.6360207844306441 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:25:02,918] Trial 76 finished with value: 0.6447345967240193 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:28:01,746] Trial 77 finished with value: 0.6245801913387635 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:32:19,654] Trial 78 finished with value: 0.6392410537775826 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:36:36,130] Trial 79 finished with value: 0.6240687904989018 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:38:48,273] Trial 80 finished with value: 0.6466047592899379 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:41:00,335] Trial 81 finished with value: 0.6264927720322329 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:44:05,511] Trial 82 finished with value: 0.6427093621562509 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:46:19,634] Trial 83 finished with value: 0.6121656469562474 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n",
      "[I 2024-03-18 17:50:46,385] Trial 84 finished with value: 0.6820897201404852 and parameters: {'architecture': [32, 64, 128], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:55:29,905] Trial 85 finished with value: 0.6308872081777629 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:57:00,589] Trial 86 finished with value: 0.6571551618330619 and parameters: {'architecture': [8, 16, 32], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 17:59:16,868] Trial 87 finished with value: 0.6151716828565387 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:01:29,253] Trial 88 finished with value: 0.6342780079473468 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:03:33,118] Trial 89 finished with value: 0.6485667614375844 and parameters: {'architecture': [16, 32, 64], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n",
      "[I 2024-03-18 18:05:36,813] Trial 90 finished with value: 0.6604623847147998 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:07:40,581] Trial 91 finished with value: 0.6550401501795825 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:09:43,529] Trial 92 finished with value: 0.6123250880662132 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:11:47,215] Trial 93 finished with value: 0.6250541999059565 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:13:50,417] Trial 94 finished with value: 0.606774768408607 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:15:54,194] Trial 95 finished with value: 0.6142155668314766 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:17:58,084] Trial 96 finished with value: 0.618337370455265 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:20:01,429] Trial 97 finished with value: 0.614276210174841 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:22:06,748] Trial 98 finished with value: 0.6397303811767522 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:24:11,750] Trial 99 finished with value: 0.6177086374338936 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:26:17,984] Trial 100 finished with value: 0.615781760391067 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:28:23,811] Trial 101 finished with value: 0.6358447942663642 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:30:28,689] Trial 102 finished with value: 0.6321957670152187 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n",
      "[I 2024-03-18 18:32:44,053] Trial 103 finished with value: 0.6903443827348597 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:34:59,351] Trial 104 finished with value: 0.6062978598124841 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 65 with value: 0.6035901544725194.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:37:14,518] Trial 105 finished with value: 0.6029359084718368 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-04.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:39:29,827] Trial 106 finished with value: 0.6439091203843846 and parameters: {'architecture': [16, 32, 64], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:41:45,198] Trial 107 finished with value: 0.6913251787807573 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 105 with value: 0.6029359084718368.\n",
      "[I 2024-03-18 18:44:00,614] Trial 108 finished with value: 0.6535784821299946 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:46:18,050] Trial 109 finished with value: 0.6564894222161349 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n",
      "[I 2024-03-18 18:48:33,361] Trial 110 finished with value: 0.6720383815905627 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n",
      "[I 2024-03-18 18:50:48,734] Trial 111 finished with value: 0.660822632996475 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:52:50,274] Trial 112 finished with value: 0.6105300740284079 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:54:46,661] Trial 113 finished with value: 0.6731774333645316 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:56:42,183] Trial 114 finished with value: 0.641531744862304 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 18:59:17,216] Trial 115 finished with value: 0.6457039937376976 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:01:10,841] Trial 116 finished with value: 0.6152502944802537 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:03:04,714] Trial 117 finished with value: 0.6125117273015135 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:04:58,687] Trial 118 finished with value: 0.6190010484527139 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-02.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:06:13,177] Trial 119 finished with value: 0.6426248011343619 and parameters: {'architecture': [8, 16, 32], 'lr': 0.1, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:08:06,700] Trial 120 finished with value: 0.6274342304643463 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:10:01,359] Trial 121 finished with value: 0.6307032822686083 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:11:54,928] Trial 122 finished with value: 0.6653379250098678 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00010: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:15:50,929] Trial 123 finished with value: 0.6209436606834916 and parameters: {'architecture': [32, 64, 128, 256], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:17:46,864] Trial 124 finished with value: 0.6261967148850945 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n",
      "[I 2024-03-18 19:19:56,908] Trial 125 finished with value: 0.6394459802438232 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00015: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:22:54,567] Trial 126 finished with value: 0.6682710840421564 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00020: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:25:04,424] Trial 127 finished with value: 0.632660573677105 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'MSELoss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:27:14,782] Trial 128 finished with value: 0.6449824549696025 and parameters: {'architecture': [16, 32, 64], 'lr': 0.01, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:29:24,725] Trial 129 finished with value: 0.6041304136023802 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00012: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00017: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:31:34,326] Trial 130 finished with value: 0.6306941596900716 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n",
      "[I 2024-03-18 19:33:46,891] Trial 131 finished with value: 0.6713565285591518 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:35:59,685] Trial 132 finished with value: 0.6079588501768953 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:38:12,551] Trial 133 finished with value: 0.6226909769808545 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-06.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:40:25,839] Trial 134 finished with value: 0.6555743296356762 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-05.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:42:41,894] Trial 135 finished with value: 0.625117175938452 and parameters: {'architecture': [16, 32, 64, 128], 'lr': 0.001, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:44:55,405] Trial 136 finished with value: 0.6213748848394436 and parameters: {'architecture': [16, 32, 64], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00011: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00016: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:48:01,360] Trial 137 finished with value: 0.6118591631598332 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00013: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00018: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:51:07,700] Trial 138 finished with value: 0.6673847187091323 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00009: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 00014: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 00019: reducing learning rate of group 0 to 1.0000e-08.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-18 19:54:10,793] Trial 139 finished with value: 0.6434160946484875 and parameters: {'architecture': [16, 32, 128, 256, 512], 'lr': 1e-05, 'optimizer': 'Adam', 'loss_function': 'L1Loss'}. Best is trial 105 with value: 0.6029359084718368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Loss:  0.6029359084718368\n",
      "  Params: \n",
      "    architecture: [16, 32, 64]\n",
      "    lr: 1e-05\n",
      "    optimizer: Adam\n",
      "    loss_function: L1Loss\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import functools\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "lambda_reg = 0.0  # for L1 regularization\n",
    "weight_decay = 0.0  # for L2 regularization\n",
    "\n",
    "\n",
    "def objective(trial, device, final_epoch):\n",
    "    # Predefined layer configurations\n",
    "    architectures = [\n",
    "        [8, 16, 32],   # Small network\n",
    "        [16, 32, 64],  # Small network\n",
    "\n",
    "        [32, 64, 128],  # Medium network\n",
    "        [16, 32, 64, 128],\n",
    "\n",
    "        [32, 64, 128, 256],  # Larger network\n",
    "        [16, 32, 128, 256, 512]  # Even larger network\n",
    "    ]\n",
    "    chosen_architecture = trial.suggest_categorical('architecture', architectures)\n",
    "    \n",
    "    model = FlexibleDual3DCNN(chosen_architecture).to(device)\n",
    "    \n",
    "    # Hyperparameters to optimize\n",
    "    lr = trial.suggest_categorical('lr', [1e-5, 1e-4, 1e-3, 1e-2, 1e-1])\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam']) #, 'RMSprop', 'SGD'])\n",
    "    loss_function = trial.suggest_categorical('loss_function', ['MSELoss', 'L1Loss']) #, 'SmoothL1Loss'])\n",
    "\n",
    "    # Select optimizer\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=4, verbose=True)\n",
    "\n",
    "\n",
    "\n",
    "    # Select loss function\n",
    "    if loss_function == 'MSELoss':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_function == 'L1Loss':\n",
    "        criterion = nn.L1Loss()\n",
    "    elif loss_function == 'SmoothL1Loss':\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(final_epoch):\n",
    "        model.train()\n",
    "        loss_list = []\n",
    "        for batch_data in train_loader:\n",
    "            pCT, rCT = batch_data[\"plan\"].to(device), batch_data[\"repeat\"].to(device)\n",
    "            reg = batch_data[\"pos\"].clone().detach().requires_grad_(True).to(device)  # If gradients are required for 'reg'\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(pCT, rCT)\n",
    "            loss = criterion(output, reg)\n",
    "\n",
    "\n",
    "            # L1 Regularization\n",
    "            l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "            for name, param in model.named_parameters():\n",
    "                # print(name, param)\n",
    "                if 'weight' in name:  # Apply L1 only to weights, not biases\n",
    "                    l1_reg = l1_reg + torch.norm(param, 1)\n",
    "            \n",
    "            # Combine loss with L1 regularization\n",
    "            loss = loss + lambda_reg * l1_reg\n",
    "\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_list.append(loss.item())\n",
    "            mean_tot_loss = np.mean(loss_list)\n",
    "            # print(f'Epoch: {epoch}/{final_epoch}, Batch: {i+1}/{len(train_loader)}, Loss_avg: {mean_tot_loss}')\n",
    "\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = []\n",
    "        with torch.no_grad():\n",
    "            for batch_data in val_loader:\n",
    "                pCT_val, rCT_val = batch_data[\"plan\"].to(device), batch_data[\"repeat\"].to(device)\n",
    "                reg_val = batch_data[\"pos\"].clone().detach().requires_grad_(True).to(device)  # If gradients are required for 'reg'\n",
    "\n",
    "                output_val = model(pCT_val, rCT_val)\n",
    "                loss_output_val = criterion(output_val, reg_val)\n",
    "\n",
    "                val_loss.append(loss_output_val.item())\n",
    "\n",
    "            mean_val_loss = np.mean(val_loss)\n",
    "            # print(f'Epoch [{epoch+1}/{final_epoch}], Validation Loss: {mean_val_loss:.4f}')\n",
    "\n",
    "            # Adjust learning rate\n",
    "            scheduler.step(mean_val_loss)\n",
    "    return mean_val_loss\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "final_epoch = 20 # The number of epochs\n",
    "\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction='minimize', study_name='DBP_optimize', storage='sqlite:///DBP_optimizer.db', load_if_exists=True)\n",
    "study.optimize(lambda trial: objective(trial, device, final_epoch), n_trials=140) # the number of trials as needed\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"  Loss: \", study.best_trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "\n",
    "# def objective(trial, model, train_loader, val_loader, device, final_epoch, mae_loss):\n",
    "#     lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "\n",
    "#     for epoch in range(starting_epoch, final_epoch):\n",
    "#         model.train()  # Set model to training mode\n",
    "#         mae_list = []\n",
    "#         train_loss = []\n",
    "#         for i, batch_data in enumerate(train_loader):  # Use enumerate to get the batch index\n",
    "#             pCT, rCT = batch_data[\"plan\"].to(device), batch_data[\"repeat\"].to(device)\n",
    "#             reg = batch_data[\"pos\"].clone().detach().requires_grad_(True).to(device)  # If gradients are required for 'reg'\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             output = model(pCT, rCT)\n",
    "#             loss_output = mae_loss(output, reg)\n",
    "\n",
    "#             loss_output.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Logging\n",
    "#             mae_list.append(loss_output.item())\n",
    "#             mean_mae = np.mean(mae_list)\n",
    "#             # Corrected to print the current batch number\n",
    "\n",
    "\n",
    "#         # Validation loop\n",
    "#         model.eval()\n",
    "#         val_loss = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch_data in val_loader:\n",
    "#                 pCT_val, rCT_val = batch_data[\"plan\"].to(device), batch_data[\"repeat\"].to(device)\n",
    "#                 reg_val = batch_data[\"pos\"].clone().detach().requires_grad_(True).to(device)  # If gradients are required for 'reg'\n",
    "\n",
    "#                 output_val = model(pCT_val, rCT_val)\n",
    "#                 loss_output_val = mae_loss(output_val, reg_val)\n",
    "\n",
    "#                 val_loss.append(loss_output_val.item())\n",
    "\n",
    "#             mean_val_loss = np.mean(val_loss)\n",
    "#     return mean_val_loss\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Use functools.partial to pass extra arguments to the objective function\n",
    "# objective_with_args = functools.partial(\n",
    "#     objective,\n",
    "#     model=model,\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     device=device,\n",
    "#     final_epoch=final_epoch,\n",
    "#     mae_loss=mae_loss,\n",
    "\n",
    "# )\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective_with_args, n_trials=5) \n",
    "\n",
    "# print(\"Best trial:\", study.best_trial.number)\n",
    "# print(\"Best value (validation loss):\", study.best_value)\n",
    "# print(\"Best hyperparameters:\", study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[I 2024-03-13 12:57:48,637] A new study created in memory with name: no-name-2cd05e81-8e1c-44d3-81a4-4be74941161f\n",
    "[I 2024-03-13 12:58:47,573] Trial 0 finished with value: 0.309533763534079 and parameters: {'lr': 9.218304386771182e-06}. Best is trial 0 with value: 0.309533763534079.\n",
    "[I 2024-03-13 12:59:54,444] Trial 1 finished with value: 0.3092626925185323 and parameters: {'lr': 6.610746908388528e-05}. Best is trial 1 with value: 0.3092626925185323.\n",
    "[I 2024-03-13 13:01:01,542] Trial 2 finished with value: 0.30921216851100325 and parameters: {'lr': 2.4698381067064442e-05}. Best is trial 2 with value: 0.30921216851100325.\n",
    "[I 2024-03-13 13:02:08,653] Trial 3 finished with value: 0.3096086760734518 and parameters: {'lr': 0.0007246235392743511}. Best is trial 2 with value: 0.30921216851100325.\n",
    "[I 2024-03-13 13:03:15,620] Trial 4 finished with value: 0.3095567206541697 and parameters: {'lr': 1.2856561858081072e-05}. Best is trial 2 with value: 0.30921216851100325.\n",
    "[I 2024-03-13 13:04:22,598] Trial 5 finished with value: 0.3095563782254855 and parameters: {'lr': 8.659948648935581e-08}. Best is trial 2 with value: 0.30921216851100325.\n",
    "[I 2024-03-13 13:05:29,773] Trial 6 finished with value: 0.3095362058530251 and parameters: {'lr': 4.637608822923589e-06}. Best is trial 2 with value: 0.30921216851100325.\n",
    "[I 2024-03-13 13:06:36,824] Trial 7 finished with value: 0.3088546040095389 and parameters: {'lr': 0.0002332795188029022}. Best is trial 7 with value: 0.3088546040095389.\n",
    "[I 2024-03-13 13:07:44,302] Trial 8 finished with value: 0.3088908504260083 and parameters: {'lr': 0.0002125349283948824}. Best is trial 7 with value: 0.3088546040095389.\n",
    "[I 2024-03-13 13:08:51,713] Trial 9 finished with value: 0.30929054742679 and parameters: {'lr': 0.0006778213291587758}. Best is trial 7 with value: 0.3088546040095389.\n",
    "Best trial: 7\n",
    "Best value (validation loss): 0.3088546040095389\n",
    "Best hyperparameters: {'lr': 0.0002332795188029022}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
