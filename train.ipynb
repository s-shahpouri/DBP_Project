{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dual_network import Dual3DCNN6 as Dual\n",
    "# from Dataset_json import PXAI_Dataset\n",
    "from decayLR import DecayLR\n",
    "\n",
    "from utilities import create_list_from_master_json, read_json_file, split_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_patient_folders(data_path):\n",
    "    \"\"\"\n",
    "    List all directories in the base_directory.\n",
    "    Each directory represents a patient.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        patient_folders = [name for name in os.listdir(data_path)\n",
    "                           if os.path.isdir(os.path.join(data_path, name))]\n",
    "        return patient_folders\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory {data_path} was not found.\")\n",
    "        return []\n",
    "\n",
    "# Specify the directory where the patient folders are located\n",
    "data_path = '/home/shahpouriz/Data/DBP_newDATA/DBP/nrrd/proton'\n",
    "\n",
    "# Get the list of patient folders\n",
    "patient_list = list_patient_folders(data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "5\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "total_patients = len(patient_list)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "seed_random = random.randint(0,100)\n",
    "train_data, val_data, test_data = split_data(patient_list, train_ratio, val_ratio, test_ratio, seed=seed_random)\n",
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "class PXAI_Dataset(Dataset):\n",
    "    def __init__(self, pCTs_path, rCTs_path, augment=False):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with lists of paths for planning CTs and repeated CTs.\n",
    "        \"\"\"\n",
    "        self.pCT_paths = pCTs_path\n",
    "        self.rCT_paths = rCTs_path\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pCT_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Since rCT_paths[idx] could be a list of paths, adjust processing accordingly\n",
    "        pCT_path = self.pCT_paths[idx]\n",
    "        rCT_paths = self.rCT_paths[idx]\n",
    "\n",
    "        pCT = self.process_scan(pCT_path)\n",
    "        rCTs = [self.process_scan(path) for path in rCT_paths]\n",
    "        reg = np.array([])  # Placeholder for registration data, adjust as needed\n",
    "\n",
    "        # Assuming rCTs is a list of rCT images, adjust the return value as per your model's requirement\n",
    "        return pCT, rCTs, reg\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def process_scan(self, path):\n",
    "        volume = self.read_nrrd_file(path)\n",
    "        if not self.dose:\n",
    "            volume = self.normalize(volume)\n",
    "        return volume\n",
    "    \n",
    "    def read_nrrd_file(self, filepath):\n",
    "        sitk_img = sitk.ReadImage(filepath)\n",
    "        img = sitk.GetArrayFromImage(sitk_img).astype(np.float32)\n",
    "        return img\n",
    "    \n",
    "    def normalize(self, volume, min=-1000, max=1000):\n",
    "        volume[ volume < min ] = min\n",
    "        volume[ volume > max ] = max\n",
    "        volume = (volume - min) / (max - min)\n",
    "        volume = volume.astype(\"float32\")\n",
    "        return volume\n",
    "    \n",
    "    def read_registration(self,path):\n",
    "        with open(path) as f:\n",
    "            lines = f.readlines()\n",
    "        params = lines[3].split(' ')\n",
    "        params = np.array(params[10::],dtype=np.float32)\n",
    "        return params\n",
    "    \n",
    "\n",
    "\n",
    "def prepare_data(data_dir, patient_ids):\n",
    "    \"\"\"\n",
    "    Scan through the patient folders to find pCT and rCT files.\n",
    "    \"\"\"\n",
    "    pct_paths = []\n",
    "    rct_paths = []\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        patient_folder = os.path.join(data_dir, patient_id)\n",
    "        # Find planning CT (pCT) and repeated CTs (rCT)\n",
    "        planning_ct = glob.glob(os.path.join(patient_folder, 'rtdose_pCT*'))\n",
    "        repeated_cts = glob.glob(os.path.join(patient_folder, 'rtdose_rCT*'))\n",
    "        \n",
    "        if planning_ct:\n",
    "            pct_paths.extend(planning_ct)\n",
    "            rct_paths.extend(repeated_cts)\n",
    "        \n",
    "    return pct_paths, rct_paths\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training, validation, and testing datasets are ready.\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "starting_epoch = 0\n",
    "decay_epoch = 150\n",
    "final_epoch = 30\n",
    "learning_rate = 0.0001\n",
    "batchsize = 5\n",
    "device_num = 1\n",
    "lambda_reg = 0.000001\n",
    "\n",
    "# Condition for saving list\n",
    "save_list = False\n",
    "\n",
    "\n",
    "exception_list = ['']\n",
    "\n",
    "# Create lists\n",
    "pct_train = []\n",
    "rct_train = []\n",
    "reg_train = []\n",
    "\n",
    "pct_val = []\n",
    "rct_val = []\n",
    "reg_val = []\n",
    "\n",
    "pct_test = []\n",
    "rct_test = []\n",
    "reg_test = []\n",
    "\n",
    "# Prepare training, validation, and testing datasets\n",
    "pct_train, rct_train = prepare_data(data_path, train_data)\n",
    "pct_val, rct_val = prepare_data(data_path, val_data)\n",
    "pct_test, rct_test = prepare_data(data_path, test_data)\n",
    "\n",
    "# Initialize datasets\n",
    "dataset_train = PXAI_Dataset(pct_train, rct_train)\n",
    "dataset_val = PXAI_Dataset(pct_val, rct_val)\n",
    "dataset_test = PXAI_Dataset(pct_test, rct_test)\n",
    "\n",
    "# Initialize DataLoader instances\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=5, shuffle=True, num_workers=1)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=5, shuffle=False, num_workers=1)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=5, shuffle=False, num_workers=1)  # If needed\n",
    "\n",
    "print(\"Training, validation, and testing datasets are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Defining loss...\n",
      "Defining optimizer...\n",
      "Defining scheduler...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Decay must start before the training session ends!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Define scheduler\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefining scheduler...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m lr_lambda \u001b[38;5;241m=\u001b[39m \u001b[43mDecayLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecay_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstep\n\u001b[1;32m     19\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLambdaLR(optimizer, lr_lambda\u001b[38;5;241m=\u001b[39mlr_lambda)\n",
      "File \u001b[0;32m/data/shahpouriz/DBP_Project/decayLR.py:4\u001b[0m, in \u001b[0;36mDecayLR.__init__\u001b[0;34m(self, epochs, offset, decay_epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs, offset, decay_epochs):\n\u001b[1;32m      3\u001b[0m     epoch_flag \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;241m-\u001b[39m decay_epochs\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (epoch_flag \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecay must start before the training session ends!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m offset\n",
      "\u001b[0;31mAssertionError\u001b[0m: Decay must start before the training session ends!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build model\n",
    "print('Initializing model...')\n",
    "model = Dual(width=128, height=128, depth=128)\n",
    "device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss\n",
    "print('Defining loss...')\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "print('Defining optimizer...')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Define scheduler\n",
    "print('Defining scheduler...')\n",
    "lr_lambda = DecayLR(epochs=final_epoch, offset=0, decay_epochs=decay_epoch).step\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Defining loss...\n",
      "Defining optimizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shahpouriz/Data/new_env/lib64/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11070). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining scheduler...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Decay must start before the training session ends!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Define scheduler\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDefining scheduler...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m lr_lambda \u001b[38;5;241m=\u001b[39m \u001b[43mDecayLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecay_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecay_epoch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstep\n\u001b[1;32m     19\u001b[0m lr_scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mLambdaLR(optimizer, lr_lambda\u001b[38;5;241m=\u001b[39mlr_lambda)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n",
      "File \u001b[0;32m/data/shahpouriz/DBP_Project/decayLR.py:4\u001b[0m, in \u001b[0;36mDecayLR.__init__\u001b[0;34m(self, epochs, offset, decay_epochs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, epochs, offset, decay_epochs):\n\u001b[1;32m      3\u001b[0m     epoch_flag \u001b[38;5;241m=\u001b[39m epochs \u001b[38;5;241m-\u001b[39m decay_epochs\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (epoch_flag \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecay must start before the training session ends!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m=\u001b[39m epochs\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moffset \u001b[38;5;241m=\u001b[39m offset\n",
      "\u001b[0;31mAssertionError\u001b[0m: Decay must start before the training session ends!"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(starting_epoch, final_epoch):\n",
    "    progress_bar = tqdm(enumerate(dataloader_train), total=len(dataloader_train))\n",
    "    mae_list = []\n",
    "    for i, data in progress_bar:\n",
    "        pCT, rCT, reg = data\n",
    "        pCT = pCT.unsqueeze(1).to(device)\n",
    "        rCT = rCT.unsqueeze(1).to(device)\n",
    "        reg = reg.to(device)\n",
    "                \n",
    "        output = model(pCT, rCT)\n",
    "        # modified L1 loss\n",
    "        # loss_output = mae_loss(output, reg)\n",
    "        loss_output = mse_loss(output, reg)\n",
    "        \n",
    "        # L1 Regularization\n",
    "        l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "        for name, param in model.named_parameters():\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        \n",
    "        loss_output += lambda_reg * l1_reg\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss_output.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mae_list.append(loss_output.item())\n",
    "        mean_mae = np.mean(mae_list)\n",
    "        progress_bar.set_description(f'Epoch: {epoch}/{final_epoch}, Batch: {i}/{len(dataloader_train)}, Loss_avg: {mean_mae.item()}', refresh=True)       \n",
    "    \n",
    "    progress_bar_val = tqdm(enumerate(dataloader_val), total=len(dataloader_val))\n",
    "    val_loss = []\n",
    "    for j, data2 in progress_bar_val:\n",
    "        pCT_val, rCT_val, reg_val = data2\n",
    "        pCT_val = pCT_val.unsqueeze(1).to(device)\n",
    "        rCT_val = rCT_val.unsqueeze(1).to(device)\n",
    "        reg_val = reg_val.to(device)\n",
    "        \n",
    "        output_val = model(pCT_val, rCT_val)\n",
    "        loss_output_val = mae_loss(output_val, reg_val)\n",
    "        # loss_output_val = mse_loss(output_val, reg_val)\n",
    "        \n",
    "        val_loss.append(loss_output_val.item())\n",
    "        mean_val_loss = np.mean(val_loss)\n",
    "        progress_bar_val.set_description(f'Epoch: {epoch}/{final_epoch}, Batch: {j}/{len(dataloader_val)}, Loss_avg: {mean_val_loss.item()}', refresh=True)\n",
    "    \n",
    "    lr_scheduler.step(mean_val_loss)\n",
    "    \n",
    "    # Save model\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    current_valid_mae = val_loss[-1]    \n",
    "    if current_valid_mae <= best_mae and epoch > 0:\n",
    "        best_mae = current_valid_mae\n",
    "        torch.save(model.state_dict(),f'{save_dir}/model_weights_dose_{epoch+1}_{fname_comment}.pt')\n",
    "    with open(loss_file, 'a') as f: #a-append\n",
    "        f.write(f'Epoch: {epoch+1}/{final_epoch}, Loss: {mean_mae}, Val: {mean_val_loss}\\n')\n",
    "            \n",
    "\n",
    "# Train patients: 31\n",
    "# ['DBP_HN011', 'DBP_HN005', 'DBP_HN012', 'DBP_HN028', 'DBP_HN041', 'DBP_HN020', 'DBP_HN043', 'DBP_HN027', 'DBP_HN006', 'DBP_HN036', 'DBP_HN024', 'DBP_HN013', 'DBP_HN032', 'DBP_HN023', 'DBP_HN014', 'DBP_HN026', 'DBP_HN039', 'DBP_HN040', 'DBP_HN035', 'DBP_HN015', 'DBP_HN025', 'DBP_HN022', 'DBP_HN031', 'DBP_HN042', 'DBP_HN033', 'DBP_HN021', 'DBP_HN018', 'DBP_HN045', 'DBP_HN037', 'DBP_HN034', 'DBP_HN002']\n",
    "# Valid patients: 8\n",
    "# ['DBP_HN004', 'DBP_HN029', 'DBP_HN007', 'DBP_HN008', 'DBP_HN010', 'DBP_HN016', 'DBP_HN017', 'DBP_HN019']\n",
    "# Test patients: 3\n",
    "# ['DBP_HN003', 'DBP_HN009', 'DBP_HN044']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
