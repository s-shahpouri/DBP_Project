{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from dual_network import Dual3DCNN6 as Dual\n",
    "# from Dataset_json import PXAI_Dataset\n",
    "from decayLR import DecayLR\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import glob\n",
    "from utilities import create_list_from_master_json, read_json_file, split_data\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_patient_folders(data_path):\n",
    "    \"\"\"\n",
    "    List all directories in the base_directory.\n",
    "    Each directory represents a patient.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        patient_folders = [name for name in os.listdir(data_path)\n",
    "                           if os.path.isdir(os.path.join(data_path, name))]\n",
    "        return patient_folders\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Directory {data_path} was not found.\")\n",
    "        return []\n",
    "\n",
    "# Specify the directory where the patient folders are located\n",
    "data_path = '/home/shahpouriz/Data/DBP_newDATA/DBP/nrrd/proton'\n",
    "\n",
    "# Get the list of patient folders\n",
    "patient_list = list_patient_folders(data_path)\n",
    "print(len(patient_list))\n",
    "print(patient_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "starting_epoch = 0\n",
    "decay_epoch = 20\n",
    "final_epoch = 30\n",
    "learning_rate = 0.0001\n",
    "batchsize = 5\n",
    "device_num = 1\n",
    "lambda_reg = 0.000001\n",
    "\n",
    "# Condition for saving list\n",
    "save_list = False\n",
    "\n",
    "\n",
    "exception_list = ['']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "\n",
    "def prepare_data(data_dir, patient_ids):\n",
    "    \"\"\"\n",
    "    Scan through the patient folders to find pCT and rCT files and match rCT files with entries in nrrd_file_info.json.\n",
    "    \"\"\"\n",
    "    pct_paths = []\n",
    "    rct_paths = []\n",
    "    reg_pos = []  \n",
    "    idxs = [\"P1\", \"P2\", \"P3\"]\n",
    "    # Load the JSON data\n",
    "    with open(os.path.join(data_dir, 'nrrd_file_info.json'), 'r') as json_file:\n",
    "        nrrd_info = json.load(json_file)\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        patient_folder = os.path.join(data_dir, patient_id)\n",
    "        file = glob.glob(os.path.join(patient_folder))\n",
    "        for idx in idxs:\n",
    "            if os.path.basename(file).split('_')[2] == idx:\n",
    "                        \n",
    "                # Find planning CT (pCT) and repeated CTs (rCT)\n",
    "                planning_ct = glob.glob(os.path.join(patient_folder, '*repeatedCT*'))\n",
    "                repeated_cts = glob.glob(os.path.join(patient_folder, '*planningCT*'))\n",
    "                \n",
    "                if planning_ct:\n",
    "                    # pct_paths.extend(planning_ct)\n",
    "                    for rct_path in repeated_cts:\n",
    "                        rct_filename = os.path.basename(rct_path)\n",
    "                        # Nested loop to match the rct_filename with repeatedCT_filename in the JSON\n",
    "                        for patient in nrrd_info:\n",
    "                            for plan_detail in patient['plan_details']:\n",
    "                                for eval_exam in plan_detail['evaluation_examinations']:\n",
    "                                    if eval_exam['repeatedCT_filename'] == rct_filename:\n",
    "                                        reg_pos.append([eval_exam['final_translation_coordinate']['x'],\n",
    "                                                        eval_exam['final_translation_coordinate']['y'],\n",
    "                                                        eval_exam['final_translation_coordinate']['z']])\n",
    "                                        rct_paths.append(rct_path)\n",
    "                                        pct_paths.append(planning_ct)\n",
    "                                        break  # Assuming unique filenames, stop searching once a match is found\n",
    "                \n",
    "            else: \n",
    "                pass\n",
    "    return pct_paths, rct_paths, reg_pos\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def prepare_data(data_dir, patient_ids):\n",
    "    pct_paths = []\n",
    "    rct_paths = []\n",
    "    reg_pos = []  \n",
    "    plan_ids = [\"P1\", \"P2\"]\n",
    "\n",
    "    # Load the JSON data\n",
    "    with open(os.path.join(data_dir, 'nrrd_file_info.json'), 'r') as json_file:\n",
    "        nrrd_info = json.load(json_file)\n",
    "    \n",
    "    for patient_id in patient_ids:\n",
    "        patient_folder = os.path.join(data_dir, patient_id)\n",
    "        all_files = glob.glob(os.path.join(patient_folder, '*.nrrd'))\n",
    "        \n",
    "        for plan_id in plan_ids:\n",
    "            planning_ct = [f for f in all_files if f.endswith(f\"{plan_id}_planningCT.nrrd\")]\n",
    "            \n",
    "            # Use regex to find repeated CT files that match the plan_id pattern\n",
    "            pattern = re.compile(f\".*{plan_id}_repeatedCT\\d*\\.nrrd\")\n",
    "            repeated_ct_files = [f for f in all_files if pattern.match(f)]\n",
    "            print(f\"Repeated CT files for {plan_id}: {repeated_ct_files}\")\n",
    "\n",
    "            # Process each planning CT file\n",
    "            if planning_ct and repeated_ct_files:\n",
    "                # print(f\"Planning CT files for {plan_id}: {planning_ct}\")\n",
    "\n",
    "                # Associate each planning CT with its corresponding repeated CTs\n",
    "                for rct_path in repeated_ct_files:\n",
    "                    rct_filename = os.path.basename(rct_path)\n",
    "                    \n",
    "                    # Look for the corresponding registration information in the JSON data\n",
    "                    for patient in nrrd_info:\n",
    "                        if patient['id'] == patient_id:\n",
    "                            for plan_detail in patient['plan_details']:\n",
    "\n",
    "\n",
    "                                for eval_exam in plan_detail['evaluation_examinations']:\n",
    "                                    # print(f\"Comparing {rct_filename} with {eval_exam['repeatedCT_filename']}\")\n",
    "                                    if rct_filename == eval_exam['repeatedCT_filename']:\n",
    "                                        # print(\"Match found\")\n",
    "                                        reg_pos.append([eval_exam['final_translation_coordinate']['x'],\n",
    "                                                        eval_exam['final_translation_coordinate']['y'],\n",
    "                                                        eval_exam['final_translation_coordinate']['z']])\n",
    "                                        rct_paths.append(rct_path)\n",
    "                                        pct_paths.append(planning_ct)\n",
    "\n",
    "                                        # Assuming unique filenames, stop searching once a match is found\n",
    "                                        break\n",
    "\n",
    "    return pct_paths, rct_paths, reg_pos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prepare training, validation, and testing datasets\n",
    "pct_train, rct_train, pos_train = prepare_data(data_path, patient_list)\n",
    "pct_val, rct_val, pos_val = prepare_data(data_path, patient_list)\n",
    "pct_test, rct_test, pos_test = prepare_data(data_path, patient_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pct_train))\n",
    "print(len(rct_train))\n",
    "print(len(pos_train))\n",
    "\n",
    "print(pct_train[:10])\n",
    "print(rct_train[:10])\n",
    "print(pos_train[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PatientDataset(Dataset):\n",
    "    def __init__(self, pct_paths, rct_paths, pos, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with paths to pCT and rCT images.\n",
    "        \"\"\"\n",
    "        self.pct_paths = pct_paths\n",
    "        self.rct_paths = rct_paths\n",
    "        self.pos = pos\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.pct_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            pct_path = self.pct_paths[idx]\n",
    "            rct_path = self.rct_paths[idx]\n",
    "            pos = self.pos[idx]\n",
    "\n",
    "            # Read and normalize the images\n",
    "            pct_img = self.read_nrrd_file(pct_path)\n",
    "            if pct_img is None:\n",
    "                raise ValueError(f\"Failed to read pCT image: {pct_path}\")\n",
    "\n",
    "\n",
    "            rct_img = self.read_nrrd_file(rct_path)\n",
    "            if pct_img is None:\n",
    "                raise ValueError(f\"Failed to read pCT image: {pct_path}\")\n",
    "            \n",
    "            pct_img = self.normalize(pct_img)\n",
    "            rct_img = self.normalize(rct_img)\n",
    "\n",
    "            # Resize and adjust spacing\n",
    "            pct_img_resized = self.resize_and_adjust_spacing(pct_img)\n",
    "            rct_img_resized = self.resize_and_adjust_spacing(rct_img)\n",
    "\n",
    "\n",
    "\n",
    "            # Convert images to PyTorch tensors and add a channel dimension\n",
    "            pct_tensor = torch.from_numpy(np.expand_dims(pct_img_resized, axis=0))\n",
    "            rct_tensor = torch.from_numpy(np.expand_dims(rct_img_resized, axis=0))\n",
    "\n",
    "\n",
    "            return pct_tensor, rct_tensor, pos\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file at index {idx}: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def read_nrrd_file(self, filepath):\n",
    "        try:\n",
    "            if isinstance(filepath[0], list):\n",
    "                filepath = filepath[0]  # Assuming the first file is the desired one if a list is provided\n",
    "            # print(f\"Filepath is a list, expected a single path. {filepath}\")\n",
    "            # print(f\"Filepath is a list, expected a single path. {filepath[0]}\")\n",
    "            sitk_img = sitk.ReadImage(filepath)\n",
    "            img = sitk.GetArrayFromImage(sitk_img).astype(np.float32)\n",
    "            if img.ndim > 3:\n",
    "                img = np.squeeze(img)\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {filepath}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def normalize(self, volume, min=-1000, max=1000):\n",
    "        \"\"\"\n",
    "        Normalizes a volume to the range [0, 1] based on specified min and max values.\n",
    "        \"\"\"\n",
    "        volume[volume < min] = min\n",
    "        volume[volume > max] = max\n",
    "        volume = (volume - min) / (max - min)\n",
    "        volume = volume.astype(\"float32\")\n",
    "        return volume\n",
    "    \n",
    "    def resize_and_adjust_spacing(self, volume, new_size=(512, 512, 512), new_spacing=(1, 1, 1)):\n",
    "        \"\"\"\n",
    "        Resizes the volume to the specified size and adjusts the spacing.\n",
    "        \"\"\"\n",
    "        sitk_img = sitk.GetImageFromArray(volume)\n",
    "\n",
    "        # Resize the image\n",
    "        sitk_resized_img = sitk.Resample(sitk_img, new_size, sitk.Transform(), sitk.sitkLinear, sitk_img.GetOrigin(),\n",
    "                                        new_spacing, sitk_img.GetDirection(), 0, sitk_img.GetPixelID())\n",
    "        \n",
    "        # Get the resized volume as a numpy array\n",
    "        resized_volume = sitk.GetArrayFromImage(sitk_resized_img).astype(np.float32)\n",
    "        \n",
    "        # # Print out the size of the resized volume\n",
    "        # print(\"Resized volume size:\", resized_volume.shape)\n",
    "        \n",
    "        # Check if the resized volume has the desired size\n",
    "        if resized_volume.shape != new_size:\n",
    "            # Handle the case where the size does not match (e.g., skip the image)\n",
    "            raise RuntimeError(f\"Resized volume size {resized_volume.shape} does not match the desired size {new_size}\")\n",
    "\n",
    "        return resized_volume\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = PatientDataset(pct_train, rct_train, pos_train)\n",
    "val_dataset = PatientDataset(pct_val, rct_val, pos_val)\n",
    "test_dataset = PatientDataset(pct_test, rct_test, pos_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for pct_tensor, rct_tensor, pos in train_dataset:\n",
    "    print(\"pCT Tensor:\", pct_tensor.shape)\n",
    "    print(\"rCT Tensor:\", rct_tensor.shape)\n",
    "    print(\"Position:\", pos)\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build model\n",
    "print('Initializing model...')\n",
    "model = Dual(width=512, height=512, depth=512)\n",
    "device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss\n",
    "print('Defining loss...')\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "print('Defining optimizer...')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Define scheduler\n",
    "print('Defining scheduler...')\n",
    "lr_lambda = DecayLR(epochs=final_epoch, offset=0, decay_epochs=decay_epoch).step\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Dual(width=512, height=512, depth=512)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Define scheduler\n",
    "lr_lambda = DecayLR(epochs=final_epoch, offset=0, decay_epochs=decay_epoch).step\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(starting_epoch, final_epoch):\n",
    "    mae_list = []\n",
    "    model.train()  # Set model to training mode\n",
    "    for i, (pCT, rCT, reg) in enumerate(train_loader):\n",
    "        pCT = pCT.to(device)\n",
    "        rCT = rCT.to(device)\n",
    "        reg = reg = reg[0].unsqueeze(1).to(device)  # Access the element of the list and move it to the device\n",
    "\n",
    "\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(pCT, rCT)\n",
    "        output = output.to(torch.float32)\n",
    "        reg = reg.to(torch.float32)\n",
    "        # Calculate loss\n",
    "        loss_output = mse_loss(output, reg)\n",
    "\n",
    "            \n",
    "        # L1 Regularization\n",
    "        l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "        for name, param in model.named_parameters():\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        loss_output += lambda_reg * l1_reg\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss_output.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        mae_list.append(loss_output.item())\n",
    "        mean_mae = np.mean(mae_list)\n",
    "        print(f'Epoch: {epoch}/{final_epoch}, Batch: {i}/{len(train_loader)}, Loss_avg: {mean_mae}')\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for j, (pCT_val, rCT_val, reg_val) in enumerate(val_loader):\n",
    "            pCT_val = pCT_val.to(device)\n",
    "            rCT_val = rCT_val.to(device)\n",
    "            reg_val = reg_val.to(device)\n",
    "            \n",
    "            output_val = model(pCT_val, rCT_val)\n",
    "            loss_output_val = mae_loss(output_val, reg_val)\n",
    "            \n",
    "            val_loss.append(loss_output_val.item())\n",
    "            mean_val_loss = np.mean(val_loss)\n",
    "            print(f'Epoch: {epoch}/{final_epoch}, Batch: {j}/{len(val_loader)}, Loss_avg: {mean_val_loss}')\n",
    "    \n",
    "    # Adjust learning rate\n",
    "    lr_scheduler.step(mean_val_loss)\n",
    "    \n",
    "    save_dir = '/home/shahpouriz/Data/DBP_Project/LOG'\n",
    "    fname_comment = 'test'\n",
    "    loss_file = fr'/home/shahpouriz/Data/DBP_Project/LOG/loss_dose_json_simpleModel_{fname_comment}.txt'\n",
    "\n",
    "    # Save model\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    current_valid_mae = val_loss[-1]    \n",
    "    if current_valid_mae <= best_mae and epoch > 0:\n",
    "        best_mae = current_valid_mae\n",
    "        torch.save(model.state_dict(),f'{save_dir}/model_weights_dose_{epoch+1}_{fname_comment}.pt')\n",
    "    with open(loss_file, 'a') as f: #a-append\n",
    "        f.write(f'Epoch: {epoch+1}/{final_epoch}, Loss: {mean_mae}, Val: {mean_val_loss}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
