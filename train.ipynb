{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from dual_network import Dual3DCNN6 as Dual\n",
    "from decayLR import DecayLR\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import glob\n",
    "from utilities import create_list_from_master_json, read_json_file, split_data\n",
    "import re\n",
    "import glob\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import list_patient_folders, prepare_data_nrrd, split_data\n",
    "\n",
    "# Specify the directory where the patient folders are located\n",
    "data_path_NEW = '/home/shahpouriz/Data/DBP_newDATA/DBP/nrrd/proton'\n",
    "data_path_OLD = '/home/shahpouriz/Data/DBP_oldDATA/nrrd/proton'\n",
    "\n",
    "# Get the list of patient folders\n",
    "\n",
    "patient_list_NEW = list_patient_folders(data_path_NEW)\n",
    "pct, rct, pos = prepare_data_nrrd(data_path_NEW, patient_list_NEW)\n",
    "data_NEW = [{\"plan\": img[0], \"repeat\": tar, \"pos\": pos} for img, tar, pos in zip(pct, rct, pos)]\n",
    "\n",
    "\n",
    "patient_list_OLD = list_patient_folders(data_path_OLD)\n",
    "pct, rct, pos = prepare_data_nrrd(data_path_OLD, patient_list_OLD)\n",
    "data_OLD = [{\"plan\": img[0], \"repeat\": tar, \"pos\": pos} for img, tar, pos in zip(pct, rct, pos)]\n",
    "\n",
    "\n",
    "# Assuming data_NEW and data_OLD are your lists of dictionaries\n",
    "data = data_NEW + data_OLD\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, test_data = split_data(data)\n",
    "\n",
    "# Check the lengths of the sets\n",
    "print(\"Number of training samples:\", len(train_data))\n",
    "print(\"Number of validation samples:\", len(val_data))\n",
    "print(\"Number of test samples:\", len(test_data))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "starting_epoch = 0\n",
    "decay_epoch = 20\n",
    "final_epoch = 30\n",
    "learning_rate = 0.0001\n",
    "batchsize = 5\n",
    "device_num = 1\n",
    "lambda_reg = 0.000001\n",
    "\n",
    "# Condition for saving list\n",
    "save_list = False\n",
    "best_mae = np.inf\n",
    "\n",
    "exception_list = ['']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### My method\n",
    "\n",
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Spacingd, SpatialPadd, CenterSpatialCropd, ScaleIntensityRanged\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "from monai.transforms import LoadImaged\n",
    "from monai.data.image_reader import ITKReader\n",
    "\n",
    "size = (256, 256, 256)\n",
    "transforms = Compose([\n",
    "        LoadImaged(keys=[\"plan\", \"repeat\"], reader=ITKReader()),\n",
    "        \n",
    "        EnsureChannelFirstd(keys=[\"plan\", \"repeat\"]),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"plan\", \"repeat\"],\n",
    "            a_min=-1000,\n",
    "            a_max=1000,\n",
    "            b_min=0.0,\n",
    "            b_max=1.0,\n",
    "        ),\n",
    "        Spacingd(keys=[\"plan\", \"repeat\"], pixdim=(2.0, 2.0, 2.0), mode='trilinear'),\n",
    "        SpatialPadd(keys=[\"plan\", \"repeat\"], spatial_size=size, mode='constant'),  # Ensure minimum size\n",
    "        CenterSpatialCropd(keys=[\"plan\", \"repeat\"], roi_size=size),  # Ensure uniform size\n",
    "    ])\n",
    "\n",
    "\n",
    "train_ds = CacheDataset(data=train_data, transform=transforms, cache_rate=1.0, num_workers=4)\n",
    "train_loader = DataLoader(train_ds, batch_size=1, shuffle=True, num_workers=4)\n",
    "\n",
    "val_ds = CacheDataset(data=val_data, transform=transforms, cache_rate=1.0, num_workers=4)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import Compose, LoadImaged, EnsureChannelFirstd, Spacingd, SpatialPadd, CenterSpatialCropd, ScaleIntensityRanged\n",
    "from monai.data import CacheDataset, DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `transforms` is defined and `val_files` contains your validation files\n",
    "check_ds = Dataset(data=val_data, transform=transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=1)\n",
    "\n",
    "# Manually retrieve the first batch of data\n",
    "for check_data in check_loader:\n",
    "    break\n",
    "\n",
    "plan, repeat = (check_data[\"plan\"][0][0], check_data[\"repeat\"][0][0])\n",
    "print(f\"image shape: {plan.shape}, target shape: {repeat.shape}\")\n",
    "\n",
    "# plot the slice [:, :, n]\n",
    "n = 100\n",
    "\n",
    "plt.figure(\"check\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(plan[:, :, n])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"repeat\")\n",
    "plt.imshow(repeat[:, :, n])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build model\n",
    "print('Initializing model...')\n",
    "model = Dual(width=512, height=512, depth=512)\n",
    "device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss\n",
    "print('Defining loss...')\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "print('Defining optimizer...')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Define scheduler\n",
    "print('Defining scheduler...')\n",
    "lr_lambda = DecayLR(epochs=final_epoch, offset=0, decay_epochs=decay_epoch).step\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(starting_epoch, final_epoch):\n",
    "    model.train()  # Set model to training mode\n",
    "    mae_list = []\n",
    "    train_loss = []\n",
    "    for i, batch_data in enumerate(train_loader):  # Use enumerate to get the batch index\n",
    "        pCT, rCT = batch_data[\"plan\"].to(device), batch_data[\"repeat\"].to(device)\n",
    "        reg = batch_data[\"pos\"].clone().detach().requires_grad_(True).to(device)  # If gradients are required for 'reg'\n",
    "\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(pCT, rCT)\n",
    "        loss_output = mse_loss(output, reg)\n",
    "            \n",
    "        # L1 Regularization\n",
    "        l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "        for name, param in model.named_parameters():\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        loss_output += lambda_reg * l1_reg\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()  \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        mae_list.append(loss_output.item())\n",
    "        mean_mae = np.mean(mae_list)\n",
    "        # Corrected to print the current batch number\n",
    "        print(f'Epoch: {epoch}/{final_epoch}, Batch: {i+1}/{len(train_loader)}, Loss_avg: {mean_mae}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch_data in val_loader:\n",
    "            pCT_val, rCT_val = batch_data[\"plan\"].to(device), batch_data[\"repeat\"].to(device)\n",
    "            reg_val = batch_data[\"pos\"].clone().detach().requires_grad_(True).to(device)  # If gradients are required for 'reg'\n",
    "\n",
    "            output_val = model(pCT_val, rCT_val)\n",
    "            loss_val = mae_loss(output_val, reg_val)\n",
    "\n",
    "            val_loss.append(loss_val.item())\n",
    "\n",
    "            mean_val_loss = np.mean(val_loss)\n",
    "            print(f'Epoch [{epoch+1}/{final_epoch}], Validation Loss: {mean_val_loss:.4f}')\n",
    "\n",
    "    # Adjust learning rate\n",
    "    lr_scheduler.step(mean_val_loss)\n",
    "    \n",
    "    save_dir = '/home/shahpouriz/Data/DBP_Project/LOG'\n",
    "    loss_file = fr'/home/shahpouriz/Data/DBP_Project/LOG/loss_dose_json_simpleModel.txt'\n",
    "\n",
    "    # Save model\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    current_valid_mae = val_loss[-1]    \n",
    "    if current_valid_mae <= best_mae and epoch > 0:\n",
    "        best_mae = current_valid_mae\n",
    "        torch.save(model.state_dict(),f'{save_dir}/model_weights_dose_{epoch+1}.pt')\n",
    "    with open(loss_file, 'a') as f: #a-append\n",
    "        f.write(f'Epoch: {epoch+1}/{final_epoch}, Loss: {mean_mae}, Val: {mean_val_loss}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for pct_tensor, rct_tensor, pos in train_dataset:\n",
    "    print(\"pCT Tensor:\", pct_tensor.shape)\n",
    "    print(\"rCT Tensor:\", rct_tensor.shape)\n",
    "    print(\"Position:\", pos)\n",
    "    count += 1\n",
    "    if count == 1:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build model\n",
    "print('Initializing model...')\n",
    "model = Dual(width=512, height=512, depth=512)\n",
    "device = torch.device(f\"cuda:{device_num}\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss\n",
    "print('Defining loss...')\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "print('Defining optimizer...')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Define scheduler\n",
    "print('Defining scheduler...')\n",
    "lr_lambda = DecayLR(epochs=final_epoch, offset=0, decay_epochs=decay_epoch).step\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = Dual(width=512, height=512, depth=512)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define loss\n",
    "mae_loss = torch.nn.L1Loss()\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "# Define scheduler\n",
    "lr_lambda = DecayLR(epochs=final_epoch, offset=0, decay_epochs=decay_epoch).step\n",
    "lr_scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(starting_epoch, final_epoch):\n",
    "    mae_list = []\n",
    "    model.train()  # Set model to training mode\n",
    "    for i, (pCT, rCT, reg) in enumerate(train_loader):\n",
    "        pCT = pCT.to(device)\n",
    "        rCT = rCT.to(device)\n",
    "        reg = reg = reg[0].unsqueeze(1).to(device)  # Access the element of the list and move it to the device\n",
    "\n",
    "\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(pCT, rCT)\n",
    "        output = output.to(torch.float32)\n",
    "        reg = reg.to(torch.float32)\n",
    "        # Calculate loss\n",
    "        loss_output = mse_loss(output, reg)\n",
    "\n",
    "            \n",
    "        # L1 Regularization\n",
    "        l1_reg = torch.tensor(0., requires_grad=True).to(device)\n",
    "        for name, param in model.named_parameters():\n",
    "            l1_reg = l1_reg + torch.norm(param, 1)\n",
    "        loss_output += lambda_reg * l1_reg\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss_output.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        mae_list.append(loss_output.item())\n",
    "        mean_mae = np.mean(mae_list)\n",
    "        print(f'Epoch: {epoch}/{final_epoch}, Batch: {i}/{len(train_loader)}, Loss_avg: {mean_mae}')\n",
    "    \n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        for j, (pCT_val, rCT_val, reg_val) in enumerate(val_loader):\n",
    "            pCT_val = pCT_val.to(device)\n",
    "            rCT_val = rCT_val.to(device)\n",
    "            reg_val = reg_val.to(device)\n",
    "            \n",
    "            output_val = model(pCT_val, rCT_val)\n",
    "            loss_output_val = mae_loss(output_val, reg_val)\n",
    "            \n",
    "            val_loss.append(loss_output_val.item())\n",
    "            mean_val_loss = np.mean(val_loss)\n",
    "            print(f'Epoch: {epoch}/{final_epoch}, Batch: {j}/{len(val_loader)}, Loss_avg: {mean_val_loss}')\n",
    "    \n",
    "    # Adjust learning rate\n",
    "    lr_scheduler.step(mean_val_loss)\n",
    "    \n",
    "    save_dir = '/home/shahpouriz/Data/DBP_Project/LOG'\n",
    "    fname_comment = 'test'\n",
    "    loss_file = fr'/home/shahpouriz/Data/DBP_Project/LOG/loss_dose_json_simpleModel_{fname_comment}.txt'\n",
    "\n",
    "    # Save model\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    current_valid_mae = val_loss[-1]    \n",
    "    if current_valid_mae <= best_mae and epoch > 0:\n",
    "        best_mae = current_valid_mae\n",
    "        torch.save(model.state_dict(),f'{save_dir}/model_weights_dose_{epoch+1}_{fname_comment}.pt')\n",
    "    with open(loss_file, 'a') as f: #a-append\n",
    "        f.write(f'Epoch: {epoch+1}/{final_epoch}, Loss: {mean_mae}, Val: {mean_val_loss}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
